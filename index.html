<!DOCTYPE html>
<html>
<head>
    <meta charset="utf-8">
    <meta name="description" content="RMTF">
    <meta name="keywords" content="Dialogue Retrieval, Vision-Language Model">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>F²RVLM</title>

    <!-- 数学公式支持 -->
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
    </script>
    <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

    <!-- 字体和样式 -->
    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">
    <link rel="stylesheet" href="./static/css/bulma.min.css">
    <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link rel="stylesheet" href="./static/css/index.css">
    <link rel="icon" href="./static/images/favicon.svg">

    <!-- JS库 -->
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script defer src="./static/js/fontawesome.all.min.js"></script>
    <script src="./static/js/bulma-carousel.min.js"></script>
    <script src="./static/js/bulma-slider.min.js"></script>
    <script src="./static/js/index.js"></script>
</head>

<body>
<!-- 页面顶部介绍区域 -->
<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h2 class="title is-2 publication-title">
            <img class="center" width="45" height="30" src="assets/F2RVLM.png">
            F²RVLM: Boosting Fine-grained Fragment Retrieval for Multi-Modal Long-form Dialogue with Vision Language Model
          </h2>
          <div class="is-size-5 publication-authors">
            <!-- 资源链接按钮 -->
            <div class="column has-text-centered">
              <div class="publication-links">
                <span class="link-block">
                  <a href="" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon"><i class="ai ai-arxiv"></i></span>
                    <span>arXiv</span>
                  </a>
                </span>
                <span class="link-block">
                  <a href="https://sprproxy-1258344707.cos.ap-shanghai.myqcloud.com/hanbobi/MLDR.tar" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon"><i class="fas fa-database"></i></span>
                    <span>MLDR Dataset</span>
                  </a>
                </span>
                <span class="link-block">
                  <a href="https://sprproxy-1258344707.cos.ap-shanghai.myqcloud.com/hanbobi/WeChat_test_set.tar" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon"><i class="fas fa-database"></i></span>
                    <span>WeChat-based Benchmark</span>
                  </a>
                </span>
                <span class="link-block">
                  <a href="https://github.com/F2RVLM/F2RVLM.github.io" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon"><i class="fab fa-github"></i></span>
                    <span>Code</span>
                  </a>
                </span>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>
  


<!-- 摘要部分 -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>Traditional dialogue retrieval aims to select the most appropriate utterance or image from recent dialogue history. However, they often fail to meet users’ actual needs for revisiting semantically coherent content scattered across long-form conversations.</p>
          <p>To fill this gap, we define the Fine-grained Fragment Retrieval (FFR) task, requiring models to locate query-relevant fragments, comprising both utterances and images, from multimodal long-form dialogues.
          As a foundation for FFR, we construct MLDR, the longest-turn multimodal dialogue retrieval dataset to date, averaging 25.45 turns per dialogue, with each naturally spanning three distinct topics. To evaluate generalization in real-world scenarios, we curate and annotate a WeChat-based test set comprising real-world multimodal dialogues with an average of 75.38 turns.
          Building on these resources, we explore existing generation-based Vision-Language Models (VLMs) on FFR and observe that they often retrieve incoherent utterance-image fragments. While optimized for generating responses from visual-textual inputs, these models lack explicit supervision to ensure semantic coherence within retrieved fragments.
          To address this, we propose F$^2$RVLM, a generative retrieval model trained in a two-stage paradigm: (1) supervised fine-tuning to inject fragment-level retrieval knowledge, and (2) GRPO-based reinforcement learning with multi-objective rewards to encourage outputs with semantic precision, relevance, and contextual coherence.
          In addition, to account for difficulty variations arising from differences in intra-fragment element distribution, ranging from locally dense to sparsely scattered, we introduce a difficulty-aware curriculum sampling that ranks training instances by predicted difficulty and gradually incorporates harder examples. This strategy enhances the model’s reasoning ability in long, multi-turn dialogue contexts.
          Experiments on both in-domain and real-domain sets demonstrate that F$^2$RVLM substantially outperforms popular VLMs, achieving superior retrieval performance.</p>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="section" id="methods">
  <hr />
  <h2 class="title is-3">Method</h2>

  <!-- Task Definition Figure -->
  <div style="text-align: center; margin-bottom: 2rem;">
    <a href="assets/Retrieval.jpg" target="_blank">
      <img src="assets/Retrieval.jpg" style="max-height: 300px; width: auto; cursor: zoom-in;" alt="FFR Task Illustration">
    </a>
    <p class="subtitle is-6">Figure 1: Comparison between traditional Dialogue Retrieval and our Fine-grained Fragment Retrieval (FFR) task.</p>
  </div>

  <!-- MLDR Construction Pipeline -->
  <div style="text-align: center; margin-bottom: 2rem;">
    <a href="assets/MLDR.jpg" target="_blank">
      <img src="assets/MLDR.jpg" style="max-height: 550px; width: auto; cursor: zoom-in;" alt="MLDR Construction Pipeline">
    </a>
    <p class="subtitle is-6">Figure 2: Overview of the MLDR construction pipeline, which integrates multimodal short dialogue processing, Qwen3-driven long-form dialogue generation, multi-granularity annotation, and retrieval-oriented task design.</p>
  </div>

  <!-- F2RVLM Framework -->
  <div style="text-align: center;">
    <a href="assets/Framework.jpg" target="_blank">
      <img src="assets/Framework.jpg" style="max-height: 550px; width: auto; cursor: zoom-in;" alt="F2RVLM Framework">
    </a>
    <p class="subtitle is-6">Figure 3: Overview of the F$^2$RVLM framework for multimodal long-form dialogue fragment retrieval. It consists of supervised fine-tuning for fragment-level knowledge injection, personalized curriculum sampling based on retrieval difficulty, and GRPO-based reinforcement learning to jointly enhance retrieval accuracy and fragment semantic coherence.</p>
  </div>
</section>

<section class="section" id="results">
  <hr />
  <h2 class="title is-3">Results</h2>

  <!-- Table Image -->
  <div style="text-align: center; margin-bottom: 2rem;">
    <a href="assets/results.png" target="_blank">
      <img src="assets/results.png" style="max-height: 500px; width: auto; cursor: zoom-in;" alt="Results Table">
    </a>
    <p class="subtitle is-6">Table 1: Comparison with popular VLMs on the MLDR validation set and WeChat test set. ``$\dagger$'' indicates zero-shot inference without MLDR fine-tuning. ``$\ast$'' indicates models limited by context length, evaluated via sliding-window inference.</p>
  </div>

  <!-- MLDR Qualitative Results -->
  <div style="text-align: center; margin-bottom: 2rem;">
    <a href="assets/MLDR_vis.jpg" target="_blank">
      <img src="assets/MLDR_vis.jpg" style="max-height: 550px; width: auto; cursor: zoom-in;" alt="MLDR Qualitative Results">
    </a>
    <p class="subtitle is-6">Figure 4: Qualitative comparison on the MLDR validation set. Given a user query, we visualize one representative case of retrieved fragments from various models. Pre-trained models often retrieve semantically relevant but incomplete or disjointed fragments. MLDR fine-tuned models improve alignment but may still miss contextual boundaries. Our F$^2$RVLM-7B achieves the most coherent and complete retrieval, accurately aligning both utterances and images with the intended semantics.</p>
  </div>

  <!-- WeChat Case 1 -->
  <div style="text-align: center; margin-bottom: 2rem;">
    <a href="assets/WeChat_vis1.jpg" target="_blank">
      <img src="assets/WeChat_vis1.jpg" style="max-height: 550px; width: auto; cursor: zoom-in;" alt="WeChat Case 1">
    </a>
    <p class="subtitle is-6">Figure 5: Case 1 of qualitative comparison on the WeChat test set, focusing on a technical-support dialogue scenario.</p>
  </div>

  <!-- WeChat Case 2 -->
  <div style="text-align: center;">
    <a href="assets/WeChat_vis2.jpg" target="_blank">
      <img src="assets/WeChat_vis2.jpg" style="max-height: 550px; width: auto; cursor: zoom-in;" alt="WeChat Case 2">
    </a>
    <p class="subtitle is-6">Figure 6: Case 2 of qualitative comparison on the WeChat test set, illustrating fragment retrieval in a personal-life dialogue scenario.</p>
  </div>
</section>



<section class="section" id="BibTeX">
<div class="container is-max-desktop content">
<h2 class="title">BibTeX</h2>
<p> If you use our work in your research, please cite: </p>
<pre><code>
@misc{anonymous2025F2RVLM,
title={F²RVLM: Boosting Fine-grained Fragment Retrieval for Multi-Modal Long-form Dialogue with Vision Language Model},
author={Anonymous},
archivePrefix={arXiv},
primaryClass={cs.CV}}
</code></pre>
</div>
</section>


<footer class="footer">
  <div class="container">
    <h2 class="title is-4 has-text-centered">References</h2> <!-- ✅ 添加标题 -->
    <div class="content has-text-centered">
      <a class="icon-link" href="#">
        <i class="fas fa-file-pdf"></i>
      </a>
    </div>
<div class="columns is-centered">
<div class="column is-8">
<div class="content">

<p>[1] Zhao Y, Huang J, Hu J, et al. Swift: a scalable lightweight infrastructure for fine-tuning[C]//Proceedings of the AAAI Conference on Artificial Intelligence. 2025, 39(28): 29733-29735.</p>

<p>[2] Hurst A, Lerer A, Goucher A P, et al. Gpt-4o system card[J]. arXiv preprint arXiv:2410.21276, 2024.</p>

<p>[3] Comanici G, Bieber E, Schaekermann M, et al. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities[J]. arXiv preprint arXiv:2507.06261, 2025.</p>
<p>[4] Guo D, Wu F, Zhu F, et al. Seed1. 5-vl technical report[J]. arXiv preprint arXiv:2505.07062, 2025</p>
<p>[5] Bai S, Chen K, Liu X, et al. Qwen2. 5-vl technical report[J]. arXiv preprint arXiv:2502.13923, 2025.</p>
<p>[6] Wu Z, Chen X, Pan Z, et al. Deepseek-vl2: Mixture-of-experts vision-language models for advanced multimodal understanding[J]. arXiv preprint arXiv:2412.10302, 2024.</p>
<hr>
</div>
</div>
</div>
</div>
</footer>


</body>
