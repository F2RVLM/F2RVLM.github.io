<!DOCTYPE html>
<html>

<head>
    <meta charset="utf-8">
    <meta name="description"
        content="F²RVLM">
    <meta name="keywords" content="Dialogue Retrieval, Vision-Language Model">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>F²RVLM</title>

    <script type="text/x-mathjax-config">
    MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
    </script>
    <script type="text/javascript"
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
        </script>

    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

    <link rel="stylesheet" href="./static/css/bulma.min.css">
    <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link rel="stylesheet" href="./static/css/index.css">
    <link rel="icon" href="./static/images/favicon.svg">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script defer src="./static/js/fontawesome.all.min.js"></script>
    <script src="./static/js/bulma-carousel.min.js"></script>
    <script src="./static/js/bulma-slider.min.js"></script>
    <script src="./static/js/index.js"></script>
</head>

<body>


<section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h2 class="title is-2 publication-title">
                <img class="center" width="45" height="30" src="assets/F2RVLM.png">
                 F²RVLM: Boosting Fine-grained Fragment Retrieval for Multi-Modal Long-form Dialogue with Vision Language Model
            </h2>
          <div class="is-size-5 publication-authors">
            <!-- 资源链接按钮 -->
            <div class="column has-text-centered">
              <div class="publication-links">
                <span class="link-block">
                  <a href="" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon"><i class="ai ai-arxiv"></i></span>
                    <span>arXiv</span>
                  </a>
                </span>
                <span class="link-block">
                  <a href="https://sprproxy-1258344707.cos.ap-shanghai.myqcloud.com/hanbobi/MLDR.tar" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon"><i class="fas fa-database"></i></span>
                    <span>MLDR Dataset</span>
                  </a>
                </span>
                <span class="link-block">
                  <a href="https://sprproxy-1258344707.cos.ap-shanghai.myqcloud.com/hanbobi/WeChat_test_set.tar" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon"><i class="fas fa-database"></i></span>
                    <span>WeChat-based Benchmark</span>
                  </a>
                </span>
                <span class="link-block">
                  <a href="https://github.com/F2RVLM/F2RVLM.github.io" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon"><i class="fab fa-github"></i></span>
                    <span>Code</span>
                    </a>
                </span>
              </div>

              </div>
  
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>
  


<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>Traditional dialogue retrieval aims to select the most appropriate utterance or image from recent dialogue history. However, they often fail to meet users’ actual needs for revisiting semantically coherent content scattered across long-form conversations.</p>
          <p>To fill this gap, we define the Fine-grained Fragment Retrieval (FFR) task, requiring models to locate query-relevant fragments, comprising both utterances and images, from multimodal long-form dialogues.
          As a foundation for FFR, we construct MLDR, the longest-turn multimodal dialogue retrieval dataset to date, averaging 25.45 turns per dialogue, with each naturally spanning three distinct topics. To evaluate generalization in real-world scenarios, we curate and annotate a WeChat-based test set comprising real-world multimodal dialogues with an average of 75.38 turns.
          Building on these resources, we explore existing generation-based Vision-Language Models (VLMs) on FFR and observe that they often retrieve incoherent utterance-image fragments. While optimized for generating responses from visual-textual inputs, these models lack explicit supervision to ensure semantic coherence within retrieved fragments.
          To address this, we propose F$^2$RVLM, a generative retrieval model trained in a two-stage paradigm: (1) supervised fine-tuning to inject fragment-level retrieval knowledge, and (2) GRPO-based reinforcement learning with multi-objective rewards to encourage outputs with semantic precision, relevance, and contextual coherence.
          In addition, to account for difficulty variations arising from differences in intra-fragment element distribution, ranging from locally dense to sparsely scattered, we introduce a difficulty-aware curriculum sampling that ranks training instances by predicted difficulty and gradually incorporates harder examples. This strategy enhances the model’s reasoning ability in long, multi-turn dialogue contexts.
          Experiments on both in-domain and real-domain sets demonstrate that F$^2$RVLM substantially outperforms popular VLMs, achieving superior retrieval performance.</p>
        </div>
      </div>
    </div>
  </div>
</section>
      <!--/ Abstract. -->


    <section class="section" id="methods">
        <hr />
        <h2 class="title is-3">Method</h2>

        <hr />
            <td align="center" style="padding-left: 0px; padding-bottom: 0px;"><img class="center" width="1280"
                    height="160" src="assets/Retrieval.jpg"></td>
        <center>
            <h5 class="title is-6">Exemplary results of animated sticker generation using our proposed approach, RDTF.
Our method performs well on different generation tasks: (top) Text&image to GIF, (middle) Prediction, (down) Interpolation.
Gray boxes indicate text or visual guidance.</h5>
        </center>

        <hr />
            <td align="center" style="padding-left: 0px; padding-bottom: 0px;"><img class="center" width="1280"
                    height="160" src="assets/MLDR.jpg"></td>
        <center>
            <h5 class="title is-6">An overview of resource-efficient multi-task training framework.
We adopt a model size that can be tolerated under constrained resources and propose a spatio temporal interaction layer to model the discreteness between animated stickers.
Furthermore, the condition mask and loss mask are designed to enable the model can effectively utilize data through multi-task and short-frame data learning, and the curriculum learning is applied to improve training of resource-constrained model.</h5>
        </center>


    </section>



    <section class="section" id="BibTeX">
        <div class="container is-max-desktop content">
          <h2 class="title">BibTeX</h2>
          <p> If you use our work in your research, please cite: </p>
          <pre><code>
            @misc{anonymous2025RDTF,
            title={Resource-Efficient Dual-Mask Training Framework for Animated Sticker Generation},
            author={Anonymous},
            archivePrefix={arXiv},
            primaryClass={cs.CV}}
      </code></pre>
        </div>
      </section>


    <footer class="footer">
        <div class="container">
            <div class="content has-text-centered">
                <a class="icon-link" href="#">
                    <i class="fas fa-file-pdf"></i>
                </a>
            </div>
            <div class="columns is-centered">
                <div class="column is-8">
                    <div class="content">
                        <p>[1] Ren, Y., Zhou, Y., Yang, J., Shi, J., Liu, D., Liu, F., ... & Shrivastava, A. (2024). Customize-A-Video: One-Shot Motion Customization of Text-to-Video Diffusion Models. CoRR.</p>

                        <p>[2] Guo, X., Zheng, M., Hou, L., Gao, Y., Deng, Y., Wan, P., ... & Ma, C. (2024, July). I2V-Adapter: A General Image-to-Video Adapter for Diffusion Models. In ACM SIGGRAPH 2024 Conference Papers (pp. 1-12).</p>

                        <p>[3] Xing, Z., Dai, Q., Hu, H., Wu, Z., & Jiang, Y. G. (2024). Simda: Simple diffusion adapter for efficient video generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 7827-7839).</p>
                        <hr>
                    </div>
                </div>
            </div>
        </div>
    </footer>


</body>
